---
title: DATA621 HW 5
author: IvanTikhonov, Seung Min Song
output:  
  html_document:
    toc: true
    toc_float: true
    show_toggle: true
  pdf_document:
  includes:
  in_header: header.html
css: ./lab.css
highlight: pygments
theme: cerulean
toc: true
toc_float: true
linkcolor: blue
date: "2023-3-12"
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(MASS)
library(car)
library(psych)
library(ggplot2)
library(hrbrthemes)
library(viridis)
library(corrplot)
library(FactoMineR)
library(VIFCP)
library(knitr)
library(kableExtra)
library(Hmisc)
library(pROC)
library(binr)
library(mice)
library(UpSetR)
options(warn=-1)
```

Overview
In this homework assignment, you will explore, analyze and model a data set containing information on
approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of
the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine
distribution companies after sampling a wine. These cases would be used to provide tasting samples to
restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a
wine to be sold at a high end restaurant. A large wine manufacturer is studying the data in order to predict the
number of wine cases ordered based upon the wine characteristics. If the wine manufacturer can predict the
number of cases, then that manufacturer will be able to adjust their wine offering to maximize sales.

## 1. Data Exploration

```{r}
train_df <- read.csv("C:/Users/SeungminSong/Downloads/621R/wine-training-data.csv")
test_df <- read.csv("C:/Users/SeungminSong/Downloads/621R/wine-evaluation-data.csv")
head(train_df)
```

### Overal Statistics
```{r}
describe(train_df)
```

Data type for all columns of a **train_df**.
```{r}
str(train_df)
```

Change int data type to numeric data type for future analysis.

```{r}
train_df$STARS <- as.numeric(train_df$STARS)
train_df$AcidIndex <- as.numeric(train_df$AcidIndex)
train_df$LabelAppeal <- as.numeric(train_df$LabelAppeal)
```

### Correlation

```{r}
cor_matrix <- cor(na.omit(train_df))

# Create the correlation plot
corrplot(cor_matrix, method = "circle", type = "upper", tl.cex = 0.8, tl.col = "black")
```

The relationship between STARS and Target appears to be positive, suggesting that higher STARS ratings may be associated with higher Target sales.
```{r}
corr_matrix <- cor(train_df[c("STARS", "TARGET", "LabelAppeal")], use = "complete.obs")
print(corr_matrix)

# print result
print(corr_matrix["TARGET", "STARS"])
print(corr_matrix["TARGET", "LabelAppeal"])
```


In regression analysis, we typically use a p-value threshold of 0.05 to determine statistical significance. Therefore, independent variables with a p-value less than 0.05 can be considered statistically significant and are likely to have a significant impact on predicting wine quality. In this model, the variables **VolatileAcidity**, **FreeSulfurDioxide**, **LabelAppeal**, and **AcidIndex** all meet this criterion and can be considered relevant in predicting wine quality.


```{r}
lm_model <- lm(STARS ~ FixedAcidity + VolatileAcidity + CitricAcid + ResidualSugar + Chlorides + ResidualSugar + Chlorides + FreeSulfurDioxide + TotalSulfurDioxide + Density + pH + Sulphates + Alcohol + LabelAppeal + AcidIndex, data = train_df)

summary(lm_model)
```

### Missing Values

Search for any missing values. Since the column STARS is an important values, it will be filled in in the data preparation step.

```{r}
colSums(is.na(train_df))
```

### Bar Plot

Bar Plot of **STARS** and **TARGET**. As you can see, the higher the STARS, the higher the average TARGET.

```{r}
TARGET_STARS_df <- train_df %>% 
  filter(!is.na(STARS)) %>%
        group_by(STARS) %>% 
        summarise(TARGET = mean(TARGET))
TARGET_STARS_df

```

```{r}

ggplot(train_df, aes(x = STARS, y = TARGET, color = STARS, fill = STARS)) +
  geom_bar(data = TARGET_STARS_df, stat = "identity", alpha = .3) + 
    #ggrepel::geom_text_repel(aes(label = STARS), color = "black", size = 2.5, segment.color = "grey") +
      #geom_point() +
        guides(color = "none", fill = "none") +
        theme_bw() +
        labs(
          title = "STARS & TARGET",
          x = "STARS",
          y = "TARGET"
        )
```

### Basic Plots

Everything looks pretty close to a normal distribution. Also, LabelAppeal, AcidIndex, and STARS are actually categorical variables.

```{r}
datasub <- reshape2::melt(train_df)
ggplot2::ggplot(datasub) +
  ggplot2::aes(x = value) + 
  ggplot2::geom_density(fill = "skyblue") + 
  ggplot2::facet_wrap(~variable, scales = 'free')

```



```{r}
# Basic Plots
# Looking at the histogram for each variable
# Margin is too large to knit on PDF
#hist.data.frame(train_df)
```


```{r}
#Well, since we have almost 13,000 observations, we have decided that we can simply remove the NAs from the data set. Except for the STARS variable. Since it takes factors, we will simply turn NA into a factor.

#NA handling
#train_df$STARS <- addNA(train_df$STARS)
#train_df <- na.omit(train_df)
#test_df$STARS <- addNA(test_df$STARS)
```

## 2. DATA PREPARATION

### Missing Value

Since STARS and TARGET have a moderate positive correlation, build the linear regression model to predict missing value for STARS. It should be noted that there are many missing values in the STARS column, which may limit the accuracy of any conclusions drawn from the analysis.

**TARGET** and **LabelAppeal** variables have a strong influence on the response variable.

```{r}
missing_STARS <- subset(train_df, is.na(STARS))

lm_model_STARS <- lm(STARS ~ TARGET + LabelAppeal, data = train_df)

predicted_STARS <- predict(lm_model_STARS, newdata = missing_STARS)

predicted_STARS_rounded <- round(predicted_STARS)

train_df$STARS[is.na(train_df$STARS)] <- predicted_STARS_rounded

summary(lm_model_STARS)

head(train_df)
```

### Data Types

Change the datatypes of a few variables. Convert specific columns to factors in separate data frames without altering the original data frames,

```{r}
str(train_df)
```

```{r}
train_df_factors <- train_df
train_df_factors$LabelAppeal <- as.factor(train_df_factors$LabelAppeal)
train_df_factors$AcidIndex <- as.factor(train_df_factors$AcidIndex)
train_df_factors$STARS <- as.factor(train_df_factors$STARS)

test_df_factors <- test_df
test_df_factors$LabelAppeal <- as.factor(test_df_factors$LabelAppeal)
test_df_factors$AcidIndex <- as.factor(test_df_factors$AcidIndex)
test_df_factors$STARS <- as.factor(test_df_factors$STARS)
```


### Box Cox

Column AcidIndex is not normally distributed. Therefore secure normality through the Box-Cox Transformation.

```{r}
ggplot(train_df, aes(x = AcidIndex)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of AcidIndex", x = "AcidIndex", y = "Frequency")

```

```{r}
# Perform Box-Cox transformation
bc_result <- boxCox(train_df$AcidIndex ~ 1, data = train_df)

# Extract lambda value
lambda <- bc_result$x[which.max(bc_result$y)]

# Apply Box-Cox transformation using the lambda value
transformed_AcidIndex <- if (lambda != 0) {
  (train_df$AcidIndex^lambda - 1) / lambda
} else {
  log(train_df$AcidIndex)
}

# Generate Q-Q plot
qqnorm(transformed_AcidIndex)
qqline(transformed_AcidIndex)

# Create a histogram of the transformed AcidIndex
ggplot(data.frame(AcidIndex = transformed_AcidIndex), aes(x = AcidIndex)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Transformed AcidIndex", x = "Transformed AcidIndex", y = "Frequency")

```





